# GPT-1
## Reimplemented GPT1 following the paper "Attention is all you need"

### Build/Pre-trained Transformer architecture used in GPT1 from scratch. The pretraining included writing mathematical expressions and code of different parts of transformer architecture such as tokens, chunks, batch data loaders, self-attention, multi-attention,  softmax, feed-forward neural network layer, and many more. I used the data set of all Harry Potter novels. This model was trained on RTX 3060 12GB GPU. With over 10,000 epochs it gave training loss of 1.05 and validation loss of 1.13. This model completed the sentence when the user gave input as shown in the output.

![transformer_arch](https://github.com/diyamth/GPT-1/assets/94824519/948b2f48-b86c-4771-b0a3-11ac2b4fb417)

